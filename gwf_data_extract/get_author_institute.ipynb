{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import xml.etree.ElementTree as et\n",
    "from lxml import etree\n",
    "import xml.dom.minidom as md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the last csv file with the file extract_from_xml_to_csv.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "# get all the authors and their institutions\n",
    "paper_url='https://api.openalex.org/works/https://doi.org/{DOI}'\n",
    "author_url = \"https://api.openalex.org/authors/{ID}\"\n",
    "\n",
    "years = range(16,24)\n",
    "results = pd.DataFrame(columns=['doi','Name','Name_Variants','Institution'])\n",
    "author_ids = []\n",
    "\n",
    "for year in years:\n",
    "    for doi in pd.read_csv(\"G\"+str(year)+\".csv\")[\"doi\"]:   \n",
    "        response = requests.get(url = paper_url.replace(\"{DOI}\", doi))\n",
    "        if response.status_code != 200:\n",
    "            print(\"not response\")\n",
    "            break\n",
    "        for author in response.json()['authorships']:\n",
    "            author_ids.append(author['author']['id'].replace(\"https://openalex.org/\",\"\"))\n",
    "            r = requests.get(url = author_url.replace(\"{ID}\", author['author']['id'].replace(\"https://openalex.org/\",\"\"))).json()\n",
    "            # names.append(r['display_name'])\n",
    "            # inst.append(r['last_known_institution']['display_name'])\n",
    "            results.loc[len(results.index)] = [doi, r['display_name'], r['display_name_alternatives'], r['last_known_institution']['display_name'] if r['last_known_institution'] else '']\n",
    "    print(year)    \n",
    "    \n",
    "results.to_csv('author_detail.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the inst_code.csv file using new author_detail.csv file\n",
    "\n",
    "author_detail = pd.read_csv('author_detail.csv')\n",
    "sorted_institution = author_detail['Institution'].value_counts().index\n",
    "pd.DataFrame({'Institution':sorted_institution, 'code':range(1, len(sorted_institution)+1)}).to_csv(\"inst_code.csv\", index=False)\n",
    "# change the position of Uwaterloo and Usaskatchewan manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a code to all records in the author_detail.csv file \n",
    "author_detail = pd.read_csv('author_detail.csv')\n",
    "inst_code = pd.read_csv('inst_code.csv')\n",
    "\n",
    "author_detail['code'] = ['0000' if pd.isna(item) else '{:04d}'.format(inst_code[inst_code['Institution'] == item]['code'].item()) if item else '' for item in author_detail['Institution']]\n",
    "author_detail.to_csv('author_detail.csv', index=False)\n",
    "\n",
    "# manually edited in this step. removed duplicates ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update xml files with unified authors (name variants corrected)\n",
    "# replace the output files with the files in data/xml/ directory\n",
    "\n",
    "author_detail = pd.read_csv('author_detail.csv')\n",
    "xml_folder = '../data/xml/'\n",
    "years = range(2016,2024)\n",
    "\n",
    "for idx, year in enumerate(years):\n",
    "    tree = et.parse(xml_folder+\"G\"+str(year)[-2:]+\".xml\")\n",
    "    root = tree.getroot()\n",
    "    for paper in root.findall(\".//paper\"):\n",
    "        for author in paper.findall(\".//author\"):\n",
    "            paper.remove(author)\n",
    "\n",
    "        for author in author_detail[author_detail['doi'] == paper.find('.//doi').text]['Name'].iloc[::-1]:\n",
    "            element = et.Element(\"author\")            \n",
    "            tag_subelement = et.SubElement(element, \"first\")\n",
    "            tag_subelement.text = author.rsplit(' ',1)[0]\n",
    "            tag_subelement = et.SubElement(element, \"last\")\n",
    "            tag_subelement.text = author.rsplit(' ',1)[1]\n",
    "            paper.insert(1,element)\n",
    "            \n",
    "\n",
    "    tree = et.ElementTree(root)\n",
    "    file_name = \"G\"+str(year)[-2:]+\".xml\"   \n",
    "    with open(file_name, \"w\", encoding=\"UTF-8\") as f:\n",
    "        f.write(etree.tostring(etree.XML(et.tostring(root, encoding=\"UTF-8\", xml_declaration=True), parser=etree.XMLParser(remove_blank_text=True))).decode())\n",
    "\n",
    "    xml_pretty_str = md.parse(file_name)\n",
    "    xml_pretty_str = xml_pretty_str.toprettyxml(encoding='UTF-8').decode()\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(xml_pretty_str)  \n",
    "\n",
    "    # print(et.tostring(root, encoding='utf8').decode('utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'canonical': {'first': 'Arash', 'last': 'Rafat'}, 'id': 'Arash-Rafat', 'comment': 'uni0006'}\n"
     ]
    }
   ],
   "source": [
    "# update name_variants.yaml\n",
    "# replace the output file with the file in data/yaml/ directory\n",
    "\n",
    "author_detail = pd.read_csv('author_detail.csv')\n",
    "\n",
    "dict_file = []\n",
    "\n",
    "for idx, item in author_detail[['Name', 'code']].drop_duplicates(subset=['Name'], keep='last').iterrows():\n",
    "    if item['Name'] == 'Arash Rafat':\n",
    "        print({'canonical' : {'first': item['Name'].rsplit(' ',1)[0], 'last': item['Name'].rsplit(' ',1)[1]}, 'id':item['Name'].rsplit(' ',1)[0].replace('.', '')+'-'+item['Name'].rsplit(' ',1)[1].replace('.', ''), 'comment':'uni{:04d}'.format(item['code'])})\n",
    "    dict_file.append({'canonical' : {'first': item['Name'].rsplit(' ',1)[0], 'last': item['Name'].rsplit(' ',1)[1]}, 'id':item['Name'].rsplit(' ',1)[0].replace('.', '')+'-'+item['Name'].rsplit(' ',1)[1].replace('.', ''), 'comment':'uni{:04d}'.format(item['code'])})\n",
    "\n",
    "\n",
    "with open(r'name_variants.yaml', 'w') as file:\n",
    "    documents = yaml.dump(dict_file, file, default_flow_style=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{{ $universities := dict \"uni0001\" \"University of Saskatchewan\" \"uni0002\" \"University of Waterloo\" \"uni0003\" \"Global Institute for Water Security\" \"uni0004\" \"McMaster University\" \"uni0005\" \"Environment and Climate Change Canada\" \"uni0006\" \"Wilfrid Laurier University\" \"uni0007\" \"University of Calgary\" \"uni0008\" \"University of Guelph\" \"uni0009\" \"Canmore Museum and Geoscience Centre\" \"uni0010\" \"Australian National University\" \"uni0011\" \"University of Montreal\" \"uni0012\" \"University of British Columbia\" \"uni0013\" \"Northern Arizona University\" \"uni0014\" \"Natural Resources Canada\" \"uni0015\" \"Universität Innsbruck\" \"uni0016\" \"Woods Hole Research Center\" \"uni0017\" \"University of Alberta\" \"uni0018\" \"Finnish Meteorological Institute\" \"uni0019\" \"University of Quebec at Montreal\" \"uni0020\" \"University of Manitoba\" \"uni0021\" \"University of Alaska Fairbanks\" \"uni0022\" \"University of Toronto\" \"uni0023\" \"Jet Propulsion Lab\" \"uni0024\" \"University of Arizona\" \"uni0025\" \"Lawrence Berkeley National Laboratory\" \"uni0026\" \"Swedish University of Agricultural Sciences\" \"uni0027\" \"National Center for Atmospheric Research\" \"uni0028\" \"University of Northern British Columbia\" \"uni0029\" \"Wageningen University & Research\" \"uni0030\" \"University of Helsinki\" \"uni0031\" \"Helmholtz Centre Potsdam - GFZ German Research Centre for Geosciences\" \"uni0032\" \"Baylor University\" \"uni0033\" \"United States Geological Survey\" \"uni0034\" \"Max Planck Institute for Biogeochemistry\" \"uni0035\" \"Agriculture and Agriculture-Food Canada\" \"uni0036\" \"San Diego State University\" \"uni0037\" \"Dalhousie University\" \"uni0038\" \"Aarhus University\" \"uni0039\" \"Grenoble Alpes University\" \"uni0040\" \"University of California, Berkeley\" \"uni0041\" \"Nanjing University of Information Science and Technology\" \"uni0042\" \"University of Ottawa\" \"uni0043\" \"National Research Council\" \"uni0044\" \"Lund University\" \"uni0045\" \"Xiamen University\" \"uni0046\" \"ETH Zurich\" \"uni0047\" \"University of Chinese Academy of Sciences\" \"uni0048\" \"Research Applications (United States)\" \"uni0049\" \"Swiss Federal Institute for Forest, Snow and Landscape Research\" \"uni0050\" \"University of Wisconsin–Madison\"}}'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# produce a line of code for 50 first universities\n",
    "# put it in the second line of hugo\\layouts\\index.html\n",
    "\n",
    "inst_code = pd.read_csv('inst_code.csv')\n",
    "\n",
    "out_str = '{{ $universities := dict'\n",
    "for idx, inst in inst_code.iloc[:50].iterrows():\n",
    "    out_str += ' \"'+'uni{:04d}'.format(inst['code'])+'\" \"'+inst['Institution']+'\"'    \n",
    "out_str += '}}' \n",
    "\n",
    "out_str"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
